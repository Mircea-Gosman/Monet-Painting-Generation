{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport statistics\nfrom PIL import Image\nfrom matplotlib import pyplot\nfrom keras.initializers import RandomNormal\nfrom keras.layers import Input, Reshape, Dropout, Dense, Flatten, BatchNormalization, Activation, ZeroPadding2D, LeakyReLU, Concatenate\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model, load_model\nfrom keras.optimizers import adam_v2\nfrom keras.utils.vis_utils import plot_model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-21T17:57:23.657148Z","iopub.execute_input":"2023-01-21T17:57:23.658134Z","iopub.status.idle":"2023-01-21T17:57:23.665970Z","shell.execute_reply.started":"2023-01-21T17:57:23.658084Z","shell.execute_reply":"2023-01-21T17:57:23.664996Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### Pre-processing","metadata":{}},{"cell_type":"code","source":"IMAGE_SIZE = 256\nIMAGE_CHANNELS = 3\nPAINTING_TRAINING_SET_SIZE = 200  # we have a total of 300 monet paintings\nPAINTINGS_DIR = '/kaggle/input/gan-getting-started/monet_jpg' \n\nreal_paintings_training = []\nreal_paintings_testing = []\n\n\nfor filename in os.listdir(PAINTINGS_DIR):\n    path = os.path.join(PAINTINGS_DIR, filename)\n    image = Image.open(path).resize((IMAGE_SIZE, IMAGE_SIZE), Image.ANTIALIAS)\n    \n    if len(real_paintings_training) < PAINTING_TRAINING_SET_SIZE:\n        real_paintings_training.append(np.asarray(image))\n    else: \n        real_paintings_testing.append(np.asarray(image))\n\nreal_paintings_training = np.reshape(\n    real_paintings_training, (-1, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS))\nreal_paintings_training = (real_paintings_training - 127.5) / 127.5\n\nreal_paintings_testing = np.reshape(\n    real_paintings_testing, (-1, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS))\nreal_paintings_testing = (real_paintings_testing - 127.5) / 127.5\n\nnp.save('/kaggle/working/real_paintings_training.npy', real_paintings_training)\nnp.save('/kaggle/working/real_paintings_testing.npy', real_paintings_testing)","metadata":{"execution":{"iopub.status.busy":"2023-01-21T18:13:49.788329Z","iopub.execute_input":"2023-01-21T18:13:49.788774Z","iopub.status.idle":"2023-01-21T18:13:51.991707Z","shell.execute_reply.started":"2023-01-21T18:13:49.788739Z","shell.execute_reply":"2023-01-21T18:13:51.990613Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n  if sys.path[0] == \"\":\n","output_type":"stream"}]},{"cell_type":"code","source":"PHOTOS_DIR = '/kaggle/input/gan-getting-started/photo_jpg'\n# We have 7000 images, but kaggle doesn't allow to store all of them in RAM :(\nKAGGLE_LIMIT = 300  \nPHOTOS_TRAINING_SET_SIZE = 200  \n\nreal_photos_training = []\nreal_photos_testing = []\n\nfor filename in os.listdir(PHOTOS_DIR):\n    if len(real_photos_training) + len(real_photos_testing) == KAGGLE_LIMIT:\n        break\n        \n    path = os.path.join(PHOTOS_DIR, filename)\n    image = Image.open(path).resize((IMAGE_SIZE, IMAGE_SIZE), Image.ANTIALIAS)\n\n    if len(real_photos_training) < PHOTOS_TRAINING_SET_SIZE:\n        real_photos_training.append(np.asarray(image))\n    else: \n        real_photos_testing.append(np.asarray(image))\n        \n\nreal_photos_training = np.reshape(\n    real_photos_training, (-1, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS))\nreal_photos_training = (real_photos_training - 127.5) / 127.5\n\nreal_photos_testing = np.reshape(\n    real_photos_testing, (-1, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS))\nreal_photos_testing = (real_photos_testing - 127.5) / 127.5\n\nnp.save('/kaggle/working/real_photos_training.npy', real_photos_training)\nnp.save('/kaggle/working/real_photos_testing.npy', real_photos_testing)","metadata":{"execution":{"iopub.status.busy":"2023-01-21T17:57:24.453042Z","iopub.status.idle":"2023-01-21T17:57:24.453435Z","shell.execute_reply.started":"2023-01-21T17:57:24.453233Z","shell.execute_reply":"2023-01-21T17:57:24.453250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Discriminator","metadata":{}},{"cell_type":"code","source":"def build_discriminator(image_shape): # PatchGAN Discriminator: https://machinelearningmastery.com/how-to-implement-pix2pix-gan-models-from-scratch-with-keras/\n    weightInit = RandomNormal(stddev=0.02)\n    input_image = Input(shape=image_shape)    \n    \n    layer = tfa.layers.SpectralNormalization(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=weightInit))(input_image)\n    layer = LeakyReLU(alpha=0.2)(layer)\n    \n    layer = tfa.layers.SpectralNormalization(Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=weightInit))(layer)\n    layer = LeakyReLU(alpha=0.2)(layer)\n    \n    layer = tfa.layers.SpectralNormalization(Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=weightInit))(layer)\n    layer = LeakyReLU(alpha=0.2)(layer)\n    \n    layer = tfa.layers.SpectralNormalization(Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=weightInit))(layer)\n    layer = LeakyReLU(alpha=0.2)(layer)\n    \n    layer = tfa.layers.SpectralNormalization(Conv2D(512, (4,4), padding='same', kernel_initializer=weightInit))(layer)\n    layer = LeakyReLU(alpha=0.2)(layer)\n    \n    layer = tfa.layers.SpectralNormalization(Conv2D(1, (4,4), padding='same', kernel_initializer=weightInit))(layer)\n    patch = Activation('sigmoid')(layer)\n    \n    model = Model(input_image, patch)\n    \n    optimizer = adam_v2.Adam(learning_rate=0.0002, beta_1=0.5)\n    model.compile(loss='mse', optimizer=optimizer, loss_weights=[0.5])\n#     model.summary()\n    plot_model(model, to_file='/kaggle/working/Cycle-GAN_discriminator.png', show_shapes=True, show_layer_names=True)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-01-21T17:57:24.455477Z","iopub.status.idle":"2023-01-21T17:57:24.456402Z","shell.execute_reply.started":"2023-01-21T17:57:24.456093Z","shell.execute_reply":"2023-01-21T17:57:24.456119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generator","metadata":{}},{"cell_type":"code","source":"def encoder_block(input_layer, num_filters):\n    weightInit = RandomNormal(stddev=0.02)\n    \n    convolution = tfa.layers.SpectralNormalization(\n        Conv2D(num_filters, kernel_size=4, strides=2, padding='same', kernel_initializer=weightInit)\n    )(input_layer)\n    activated_convolution = LeakyReLU(alpha=0.2)(convolution)\n    \n    return activated_convolution\n\ndef decoder_block(input_layer, layer_to_skip_to, num_filters, dropout=True):\n    weightInit = RandomNormal(stddev=0.02)\n    \n    transposed_convolution = tf.keras.layers.Conv2DTranspose(\n        num_filters, kernel_size=4, strides=2, padding='same', kernel_initializer=weightInit\n    )(input_layer)\n    activated_transposed_convolution = BatchNormalization(momentum=0.1)(transposed_convolution)\n    \n    if dropout:\n        activated_transposed_convolution = Dropout(0.5)(activated_transposed_convolution, training=True)\n    \n    skip_connection = Concatenate()([activated_transposed_convolution, layer_to_skip_to])\n    activated_skip_connection = Activation('relu')(skip_connection)\n    \n    return activated_skip_connection\n\ndef build_generator(image_shape):\n    weightInit = RandomNormal(stddev=0.02)\n    \n    input_image = Input(shape=image_shape)\n    \n    encoder_64  = encoder_block(input_image, 64)\n    encoder_128 = encoder_block(encoder_64, 128)\n    encoder_256 = encoder_block(encoder_128, 256)\n    encoder_512_a = encoder_block(encoder_256, 512)\n    encoder_512_b = encoder_block(encoder_512_a, 512)\n    encoder_512_c = encoder_block(encoder_512_b, 512)\n    encoder_512_d = encoder_block(encoder_512_c, 512)\n    \n    bottleneck_convolution = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=weightInit)(encoder_512_d)\n    bottleneck_activation = Activation('relu')(bottleneck_convolution)\n    \n    decoder_512_d = decoder_block(bottleneck_activation, encoder_512_d, 512)\n    decoder_512_c = decoder_block(decoder_512_d, encoder_512_c, 512)\n    decoder_512_b = decoder_block(decoder_512_c, encoder_512_b, 512)\n    decoder_512_a = decoder_block(decoder_512_b, encoder_512_a, 512, dropout=False)\n    decoder_256 = decoder_block(decoder_512_a, encoder_256, 256, dropout=False)\n    decoder_128 = decoder_block(decoder_256, encoder_128, 128, dropout=False)\n    decoder_64 = decoder_block(decoder_128, encoder_64, 64, dropout=False)\n    \n    output = tf.keras.layers.Conv2DTranspose(3, kernel_size=4, strides=2, padding='same', kernel_initializer=weightInit)(decoder_64)\n    activated_output = Activation('tanh')(output)\n    \n    model = Model(input_image, activated_output)\n#     model.summary()\n    plot_model(model, to_file='/kaggle/working/Cycle-GAN_generator.png', show_shapes=True, show_layer_names=True)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-01-21T17:57:24.457933Z","iopub.status.idle":"2023-01-21T17:57:24.459116Z","shell.execute_reply.started":"2023-01-21T17:57:24.458829Z","shell.execute_reply":"2023-01-21T17:57:24.458856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GAN","metadata":{}},{"cell_type":"code","source":"# Test function to build models into one another, \n# [ This model would work for normal GAN, but not for Cycle, so it's not used. ]\ndef build_gan(gen_paintings_model, gen_photos_model, disc_paintings_model, disc_photos_model, image_shape):\n    in_real_painting = Input(shape=image_shape)\n    in_real_photo = Input(shape=image_shape)\n    \n    generated_painting = gen_paintings_model(in_real_photo)\n    disc_paintings = disc_paintings_model([in_real_painting, generated_painting])\n    \n    generated_photos = gen_photos_model(in_real_painting)\n    disc_photos = disc_photos_model([in_real_photo, generated_photos])\n    \n    gan_model = Model(\n        [in_real_painting, in_real_photo], \n        [generated_painting, disc_paintings, generated_photos, disc_photos],\n        name=\"Cycle GAN\"\n    )\n    \n    optimizer = Adam(lr=0.0002, beta_1=0.5)\n    gan_model.compile(loss=['mse', 'mae', 'mae', 'mae'], optimizer=optimizer, loss_weights=[1, 5, 10, 10])\n    \n    return gan_model\n\n# We train generators separately for each direction of the cycle to implement cycle loss.\ndef composite_model(gen_model_1, disc_model, gen_model_2, image_shape):\n    gen_model_1.trainable = True\n    disc_model.trainable = False\n    gen_model_2.trainable = False\n\n    input_gen = Input(shape=image_shape)\n    gen_1_output = gen_model_1(input_gen)\n    disc_output = disc_model(gen_1_output)\n\n    identity_input = Input(shape=image_shape)\n    identitiy_output = gen_model_1(identity_input)\n\n    forward_output = gen_model_2(gen_1_output)\n\n    gen_2_output = gen_model_2(identity_input)\n    backward_output = gen_model_1(gen_2_output)\n\n    model = Model([input_gen, identity_input], [disc_output, identitiy_output, forward_output, backward_output])\n    optimizer = adam_v2.Adam(learning_rate=0.0002, beta_1=0.5)\n    model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=optimizer)\n\n    return model\n\n# Grab real samples & their target classification from disc. POV (Tensor of 1s)\ndef generate_real_samples(dataset, n_samples, patch_shape):\n    ix = np.random.randint(0, dataset.shape[0], n_samples)\n    X = dataset[ix]\n    y = np.ones((n_samples, patch_shape, patch_shape, 1))\n \n    return X, y\n\n# Grab fake samples & their target classification from disc. POV (Tensor of 0s)\ndef generate_fake_samples(g_model, dataset, patch_shape):\n    X = g_model.predict(dataset)\n    y = np.zeros((len(X), patch_shape, patch_shape, 1))\n    \n    return X, y\n\n# The reference material keeps a pool of 50 images to compute loss\n# [ Instead of computing loss for each newly generated image, we get an average. ]\n# [ This reduces gradient yo-yo-ing. ]\ndef update_image_pool(pool, images, max_size=50):\n    selected = list()\n    for image in images:\n        if len(pool) < max_size:\n            # stock the pool\n            pool.append(image)\n            selected.append(image)\n        elif random.uniform(0, 1) < 0.5:\n            # use image, but don't add it to the pool\n            selected.append(image)\n        else:\n            # replace an existing image and use replaced image\n            ix = np.random.randint(0, len(pool))\n            selected.append(pool[ix])\n            pool[ix] = image\n    return np.asarray(selected)","metadata":{"execution":{"iopub.status.busy":"2023-01-21T17:57:24.460421Z","iopub.status.idle":"2023-01-21T17:57:24.461394Z","shell.execute_reply.started":"2023-01-21T17:57:24.461101Z","shell.execute_reply":"2023-01-21T17:57:24.461125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Models","metadata":{}},{"cell_type":"code","source":"image_shape = (256,256,3)\n\npainting_generator = build_generator(image_shape)   # Creates paintings\nphoto_generator = build_generator(image_shape)      # Creates photos \npainting_disc = build_discriminator(image_shape)    # Classifies paintings\nphoto_disc = build_discriminator(image_shape)       # Classifies photos\n\npainting_to_photo = composite_model(painting_generator, painting_disc, photo_generator, image_shape)\nphoto_to_painting = composite_model(photo_generator, photo_disc, painting_generator, image_shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-21T17:57:24.462528Z","iopub.status.idle":"2023-01-21T17:57:24.463410Z","shell.execute_reply.started":"2023-01-21T17:57:24.463127Z","shell.execute_reply":"2023-01-21T17:57:24.463152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Helper to save images during training / predicting afterwards","metadata":{}},{"cell_type":"code","source":"def save_images(step, data, isPainting, subpath=\"\"):   \n    output_path = '/kaggle/working/output' + subpath\n    tag = 'paint'\n    \n    # GAN algorithmes compute values from [ 0, 1 ], so we bring it back to [ 0, 256 ]\n    image = data  * 127.5 + 127.5\n    image = (np.rint(image)).astype(np.uint8)\n\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n      \n    # Function is used to save paitings & photos\n    if not isPainting:\n        tag = 'photo'\n        \n    filename = os.path.join(output_path, f\"trained-{step}-{tag}.png\")\n    im = Image.fromarray(image[0])\n    im.save(filename)","metadata":{"execution":{"iopub.status.busy":"2023-01-21T17:57:24.464797Z","iopub.status.idle":"2023-01-21T17:57:24.465639Z","shell.execute_reply.started":"2023-01-21T17:57:24.465387Z","shell.execute_reply":"2023-01-21T17:57:24.465412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset):\n    n_epochs, n_batch, = 10,1                # For more complex horse->zebra: 100, 1 \n    n_patch = d_model_A.output_shape[1]      # determine the output square shape of the discriminator   \n    trainA, trainB = dataset                \n    poolA, poolB = list(), list()            \n    bat_per_epo = int(len(trainA) / n_batch) # calculate the number of batches per training epoch\n    n_steps = bat_per_epo * n_epochs         # calculate the number of training iterations\n    print(f\"n_steps: {n_steps}\")             \n    \n    # manually enumerate epochs\n    for i in range(n_steps):\n        print(f\"Doing Step: {i}\")\n        # select a batch of real samples\n        X_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch)\n        X_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch)\n        \n        # generate a batch of fake samples\n        X_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\n        X_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\n        \n        # Save Outputs\n        save_images(i, X_fakeA, True)\n        save_images(i, X_realB, False)\n        \n        # update fakes from pool\n        X_fakeA = update_image_pool(poolA, X_fakeA)\n        X_fakeB = update_image_pool(poolB, X_fakeB)\n        \n        # update generator B->A via adversarial and cycle loss\n        g_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n        \n        # update discriminator for A -> [real/fake]\n        dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n        dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n        \n        # update generator A->B via adversarial and cycle loss\n        g_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n        \n        # update discriminator for B -> [real/fake]\n        dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n        dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n\ntrain(painting_disc, photo_disc, painting_generator, photo_generator, painting_to_photo, photo_to_painting, [real_paintings_training, real_photos_training])","metadata":{"execution":{"iopub.status.busy":"2023-01-21T17:57:24.467098Z","iopub.status.idle":"2023-01-21T17:57:24.468759Z","shell.execute_reply.started":"2023-01-21T17:57:24.468456Z","shell.execute_reply":"2023-01-21T17:57:24.468485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SWD","metadata":{}},{"cell_type":"code","source":"def sliced_wasserstein(A, B, dir_repeats=128, dirs_per_repeat=4):\n    \n    A = np.reshape(A, (A.shape[0], -1))\n    B = np.reshape(B, (B.shape[0], -1))\n    \n    assert A.ndim == 2 and A.shape == B.shape                           # (neighborhood, descriptor_component)\n    results = []\n    for repeat in range(dir_repeats):\n        dirs = np.random.randn(A.shape[1], dirs_per_repeat)             # (descriptor_component, direction)\n        dirs /= np.sqrt(np.sum(np.square(dirs), axis=0, keepdims=True)) # normalize descriptor components for each direction\n        dirs = dirs.astype(np.float32)\n        projA = np.matmul(A, dirs)                                      # (neighborhood, direction)\n        projB = np.matmul(B, dirs)\n        projA = np.sort(projA, axis=0)                                  # sort neighborhood projections for each direction\n        projB = np.sort(projB, axis=0)\n        dists = np.abs(projA - projB)                                   # pointwise wasserstein distances\n        results.append(np.mean(dists))                                  # average over neighborhoods and directions\n    return np.mean(results)","metadata":{"execution":{"iopub.status.busy":"2023-01-21T18:01:01.363969Z","iopub.execute_input":"2023-01-21T18:01:01.364398Z","iopub.status.idle":"2023-01-21T18:01:01.375227Z","shell.execute_reply.started":"2023-01-21T18:01:01.364359Z","shell.execute_reply":"2023-01-21T18:01:01.373860Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"### Predict some more results with trained generators","metadata":{}},{"cell_type":"code","source":"### Grab random photos or paintings and convert them to the other style\n\npatch_shape = painting_disc.output_shape[1]\n\nphotos_to_paintings_SWD = []\npaintings_to_photos_SWD = []\n\n# Photo to Paintings\nfor i in range(len(real_photos_testing)):\n    ix = np.random.randint(0, real_photos_testing.shape[0], patch_shape)\n    data = real_photos_testing[ix]\n    image = photo_generator.predict(data)\n#     save_images(i, data, False, '/generated_paitings')\n#     save_images(i, image, True, '/generated_paitings')\n    photos_to_paintings_SWD.append(sliced_wasserstein(data, image))\n    \n    # Stop Early if needed\n    print(i)\n    if i == 15:\n        break\n\n# Paintings to Photo \nfor i in range(len(real_paintings_testing)):\n    ix = np.random.randint(0, real_paintings_testing.shape[0], patch_shape)\n    data = real_paintings_testing[ix]\n    image = painting_generator.predict(data)\n#     save_images(i, data, False, '/generated_photos')\n#     save_images(i, image, True, '/generated_photos')   \n    paintings_to_photos_SWD.append(sliced_wasserstein(data, image))\n    \n    # Stop Early if needed\n    print(i)\n    if i == 15:\n        break\n    \n\nprint(f\"Mean SWD for photos to paintings: {statistics.mean(photos_to_paintings_SWD)}\")\nprint(f\"Mean SWD for paintings to photos: {statistics.mean(paintings_to_photos_SWD)}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-01-21T18:14:25.095941Z","iopub.execute_input":"2023-01-21T18:14:25.096377Z","iopub.status.idle":"2023-01-21T18:19:41.025221Z","shell.execute_reply.started":"2023-01-21T18:14:25.096331Z","shell.execute_reply":"2023-01-21T18:19:41.024041Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nMean SWD for photos to paintings: 0.06677430730273845\nMean SWD for paintings to photos: 0.06010330965283581\n","output_type":"stream"}]},{"cell_type":"code","source":"# Download data to view locally to view photos (Kaggle doesn't allow that)\nos.chdir(r'/kaggle/working')\n\n!tar -czf Out.tar.gz ./\n\nfrom IPython.display import FileLink\n\nFileLink(r'Out.tar.gz')","metadata":{"execution":{"iopub.status.busy":"2023-01-21T17:57:24.474683Z","iopub.status.idle":"2023-01-21T17:57:24.475483Z","shell.execute_reply.started":"2023-01-21T17:57:24.475211Z","shell.execute_reply":"2023-01-21T17:57:24.475236Z"},"trusted":true},"execution_count":null,"outputs":[]}]}